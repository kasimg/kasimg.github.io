{"meta":{"title":"Kasim","subtitle":"Kasim","description":"","author":"Kasim","url":"kasimg.github.io","root":"/"},"pages":[{"title":"EM算法","date":"2020-03-05T01:17:45.000Z","updated":"2020-03-05T09:32:08.773Z","comments":true,"path":"to_be_posted/EM算法.html","permalink":"kasimg.github.io/to_be_posted/EM%E7%AE%97%E6%B3%95.html","excerpt":"","text":""},{"title":"Hello World","date":"2020-03-19T05:04:33.275Z","updated":"2019-12-02T06:05:53.042Z","comments":true,"path":"to_be_posted/hello-world.html","permalink":"kasimg.github.io/to_be_posted/hello-world.html","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment"},{"title":"test","date":"2019-12-02T08:08:41.000Z","updated":"2020-02-27T05:18:09.733Z","comments":true,"path":"to_be_posted/test.html","permalink":"kasimg.github.io/to_be_posted/test.html","excerpt":"","text":"正儿八经学技术 花里胡哨瞎折腾 七嘴八舌侃天地 多愁善感拾回忆"},{"title":"scala","date":"2020-03-04T08:48:53.000Z","updated":"2020-03-13T09:28:06.171Z","comments":true,"path":"to_be_posted/scala.html","permalink":"kasimg.github.io/to_be_posted/scala.html","excerpt":"","text":"若函数没有参数，则调用时可以不加括号，类似于计算属性和js中的get方法 若有返回值，则最后一行作为结果 默认参数类似es6和python 关键字参数类似python for循环中加入条件判断 for (i &lt;- Range(0, 10) if i % 2 ==0) { println(i)} class中直接放入代码段 不显示定义array list等中的类型，则可以多类型并存 “hello”(4)是”hello”.apply(4)的简写"}],"posts":[{"title":"决策树和随机森林","slug":"决策树","date":"2020-03-03T05:31:04.000Z","updated":"2020-03-19T04:48:03.594Z","comments":true,"path":"2020/03/03/决策树/","link":"","permalink":"kasimg.github.io/2020/03/03/%E5%86%B3%E7%AD%96%E6%A0%91/","excerpt":"年龄太大？不见 长得不好看？不见 收入太低？不见 … 人们在相亲的时候难免会做出以上的决策，当然，爱美之心人皆有之，而且人人都有追求更好物质生活的精神生活的权利。","text":"年龄太大？不见 长得不好看？不见 收入太低？不见 … 人们在相亲的时候难免会做出以上的决策，当然，爱美之心人皆有之，而且人人都有追求更好物质生活的精神生活的权利。 1. 什么是决策树如果把相亲的决策过程可视化，可以得到下面的图： 图1. 可视化决策过程图中黄色部分表示判断依据，依次按照年龄、长相、收入、是否是公务员4个方面进行决策，最终得出“见”或者“不见”的结论。 此过程长得很像一棵树，所以我们把这种决策方法叫做决策树。 2. 使用决策树2.1 分类问题下面看一个比较具体的问题，下表记录了两个星期内的天气、湿度、风级状况，以及小明是否外出打球： 日期 天气 湿度 风级 是否打球 1 晴 高 弱 否 2 晴 高 强 否 3 阴 高 弱 是 4 雨 高 弱 是 5 雨 正常 弱 是 6 雨 正常 强 否 7 阴 正常 强 是 8 晴 高 弱 否 9 晴 正常 弱 是 10 雨 正常 弱 是 11 晴 正常 强 是 12 阴 高 强 是 13 阴 正常 弱 是 14 雨 高 强 否 现在要求是给出任意的天气、湿度、风级条件，预测出小明是否会出门打球。 2.2 信息熵我们希望画出一课类似图1中的决策树，那么首先需要找到一个特征作为判断依据，这里的特征有三个： 天气 湿度 风级 显然，先选天气作为判断依据，和先选湿度作为判断依据，得到的决策树肯定是不同的。那么既然能得到很多颗决策树，那一颗数比较好呢？ 下面先按照天气划分： 图2. 按照天气划分如何衡量这种划分策略的好坏？ 以所有晴天的数据为例，一共有5个晴天，其中2天小明出去打球，3天没有，那么可以认为在天晴时，小明外出打球的概率为： P(打球|天晴)=\\frac2{2+3}=0.4 \\tag 1考虑下面这个函数： H(p)=-plnp-(1-p)ln(1-p) \\tag 2其中p为事件A发生的概率，这个函数的图像为： 图3. 信息熵图像可以看到，当p为0时，表示事件A不可能发生，此时函数值为0，同样的，当p为1时，表示事件A一定发生，函数值为0。而当p为0.5时，表示事件A发生的概率和不发生的概率相同，函数值达到最大。 这个函数可以用来度量事件的不确定性。当p为0或1时，确定发生或者不发生，确定性最大，所以不确定性最小；反之，p为0.5时不确定性最大。 回到天气划分上，天晴的情况下，小明出门打球的概率为0.4，不确定性比较高，而我们希望不确定性越低越好，从而得到更加精确的决策结果。 式（2）中的H(p)叫做信息熵，用于度量信息的不确定性。 这样一来，我们选择某个特征的标准就是： 特征的熵最小 2.3 特征的信息熵令“小明出门打球”为事件A，天气特征为C，其中： C=\\{C_1,C_2,...,C_n\\} \\tag 3Ci为特征的所有可能取值。 在Ci情况下的信息熵为： H(A,C_i)=-P(A|C_i)*lnP(A|C_i)-\\big(1-P(A|C_i)\\big)ln\\big(1-P(A|C_i)\\big) \\tag 4而这仅仅是特征中一个取值的熵，那么特征C的总信息熵可以定义为： H(A,C)=\\sum_{i=1}^nH(A,C_i)\\cdot w_i \\tag 5其中ωi为特征中各个取值Ci的权重，假设总样本量为n，有j个样本取值为Ci，那么： w_i=\\frac jn这样一来，就可以度量每一个特征的不确定性，选择其中不确定性最小的一个，座位此次划分的标准。 此策略是贪心策略，并不能保证全局最优 2.4 信息增益和信息增益比信息增益定义为： Gain(A, C)=H(A)-H(A,C) \\tag 6表示以C作为判别条件，新的信息熵相较于之前的信息熵下降的幅度。显然，信息增益越大，表示不确定性下降的越多，说明特征选取越好。 现在假设有n个样本，条件C有n中取值，将样本分为n类，每类只有一个样本。那么这种情况下，特征C的熵为0，因为每种取值的熵都为0。这是我们不愿意看到的情况：过拟合 如果以信息增益为判断标准，那么这种可能导致过拟合的特征因为其信息增益最大，所以首当其冲。这并不是我们想看到的。 于是给这些特征一个限制条件，定义信息增益比： GainRatio(A, C)=\\frac {Gain(A,C)}{H(A, C)} \\tag 7将信息增益比作为新的判断条件，需要给出合适的区间（比值太大和太小都不行），这样能排除掉会过拟合的情况，也能保证信息增益较高。 2.5 特征连续的决策边界选取上面的讨论是建立在特征的取值属于一个集合的情况，那么如果特征取值是一个数值（特征连续）该如何？ 看下图： 图4. 决策边界选取有4个样本点，点关于特征C的取值为x1到x4，样本点两两之间的距离为d1，d2，d3。 两个相邻的样本点之间不存在样本点，所以相邻间隔区域的中点b1，b2，b3可以作为候选决策边界。在b1，b2，b3中选择不确定性最小的边界作为最终的决策边界。 这样有一个问题，如果样本数量非常多，那么候选决策边界也会非常多，要是计算每个候选决策边界的熵值，计算成本太高。所以通常的做法是随机选取若干个候选决策边界，找出其中最好的一个。 3. 随机森林为了解决过拟合问题，现采用有放回的采样方式。 假设有样本： X=\\{(X_1,y_1),(X_2,y_2),...,(X_n,y_n)\\}从中有放回地采样m次, 每次取k个，每次的样本为： X_m=\\{(X_{m1},y_{m1}),(X_{m2}y_{m2}),...,(X_{mk},y_{mk})\\}每次的采样结果对应一个决策树DTm，这些决策树组成了一个集合： RF(X)=\\{DT_1,DT_2,...,DT_m\\}这个集合叫做随机森林。 当一个测试点进入随机森林之后，会通过森林中所有的决策树，每棵决策树对应一个分类结果。然后根据少数服从多数的原则，分类结果中频数最高的为最终分类结果。 4. 总结 决策树/随机森林的逻辑比较简单，也许效果不是最好，但是可以作为对数据分布探索的首要尝试算法 没有谈到代码实现部分，以后会补上","categories":[{"name":"正儿八经学技术","slug":"正儿八经学技术","permalink":"kasimg.github.io/categories/%E6%AD%A3%E5%84%BF%E5%85%AB%E7%BB%8F%E5%AD%A6%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"kasimg.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"对SVM的一些理解","slug":"SVM","date":"2020-02-29T07:04:27.000Z","updated":"2020-03-19T04:44:05.439Z","comments":true,"path":"2020/02/29/SVM/","link":"","permalink":"kasimg.github.io/2020/02/29/SVM/","excerpt":"20世纪90年代曾经制霸机器学习届的SVM，虽然如今风光不再，但是其思想和推导过程仍十分值得学习，所谓温故而知新。","text":"20世纪90年代曾经制霸机器学习届的SVM，虽然如今风光不再，但是其思想和推导过程仍十分值得学习，所谓温故而知新。 1. 更好地分类先看一个简单的分类问题： 图1. 简单的二分类问题如上图所示，-表示负样本，+表示正样本。 有三条分界线a，b，c，直观上来看，将b作为分界线效果更好，因为这样容错率更高。 假设以a为分界线，那么如果负样本观测值因为噪声存在而有所偏差，则很有可能会将这个负样本误判为正样本；同理，以c为分界线也会导致相同的问题。 那么可以这么理解：决策边界的容错率更高，鲁棒性更好，那么我们认为这个边界效果更好。 2. 让街道最宽现在来寻找一个最好的决策边界，体现在图1中就是： 决策边界离样本点尽可能远 公平起见，距离正负样本距离相同 也就是说，要使下图的阴影部分宽度最大： 图2. 阴影部分为了方便起见，后面将这里的阴影部分称作街道。 那么如何让街道最宽呢？ 现有直线L： \\begin{split} &L:\\omega^Tx+b=0 \\\\ &\\omega=(\\omega_1,\\omega_2)^T \\\\ &x=(x_1, x_2)^T \\end{split} \\tag 1平移直线L，记下L最后一次与负样本相交的位置，以及第一次与正样本相交的位置，那么L在这两个对应的直线为M，N： \\begin{split} M&:\\omega^Tx+b=m \\\\[2ex] N&:\\omega^Tx+b=n \\end{split} \\tag 2那么此时MN之间的距离就是街宽。 接着找到直线B，使得B到M和N的距离相等，令B： B:\\omega^Tx+b = 0 \\tag 3那么M，N为： \\begin{split} M&:\\omega^Tx+b=-k \\\\[2ex] N&:\\omega^Tx+b=k \\end{split}具体情形见下图： 图3. 最大街宽此时的决策边界就是直线B。 两边同时乘相同的值，直线解析式不变，对于M和N，两边同时乘1/k，可以得到： \\begin{split} M&:\\frac{\\omega^Tx - b}k = -1 \\\\[2ex] N&:\\frac{\\omega^Tx - b}k = 1 \\\\[2ex] \\end{split} \\tag 4令： (\\omega^T)'=\\frac{\\omega^T}k \\ \\ \\ , \\ \\ \\ b'=\\frac bk有： \\begin{split} M'&:(\\omega^T)'x+b'=-1 \\\\[2ex] N'&:(\\omega^T)'x+b'=1 \\end{split} \\tag 5显然，M和M’以及N和N’是同样的直线，这里为了表示方便，依然用原来的参数，所以： \\begin{split} M&:\\omega^Tx+b=-1 \\\\[2ex] N&:\\omega^Tx+b=1 \\end{split}PS：这里使直线解析式右侧为1/-1有两个原因。第一，如果不加以限制，等比缩放ω和b的话，有无数种可能的解析式，所以这里将其限制住，使得只能得出一种解析式。第二，固定为1/-1是为了方便计算。 落在M和N上的点成为支持向量 获得街道区域之后，不难发现，此时训练集中的样本满足： \\left \\{ \\begin{array}{c} \\omega^Tx+b\\ge1,\\ \\ \\ \\ \\ \\ y=1 \\\\[3ex] \\omega^Tx+b\\le-1,\\ \\ \\ \\ y=-1 \\end{array} \\right. \\tag 6令： y_i= \\left \\{ \\begin{array}{c} 1，x_i为正样本 \\\\[3ex] -1，x_i为负样本 \\end{array} \\right. \\tag 7则样本满足： y_i(\\omega^Tx+b)-1\\ge0 \\tag 83. 计算道路宽度如图： 图4. 计算街道宽度其中A，B分别是直线M，N上的点，过点A做直线N的垂线交N于点P，那么街道宽度AP的长度为： \\begin{split} AP&=\\vec {AB}\\cdot \\frac {\\omega^T}{|\\omega^T|} \\\\ &=(\\vec{OB}-\\vec{OA})\\cdot \\frac {\\omega^T}{|\\omega^T|} \\\\ &=\\vec{OB}\\cdot \\frac {\\omega^T}{|\\omega^T|}-\\vec{OA}\\cdot \\frac {\\omega^T}{|\\omega^T|} \\end{split} \\tag 9因为A，B在M，N上，所以有： \\left \\{ \\begin{array}{c} \\omega^T \\cdot\\vec {OB} + b-1=0 \\\\ -\\omega^T \\cdot\\vec {OA} - b-1=0 \\end{array} \\right. \\tag {10}即： \\left \\{\\begin{array}{c} \\omega^T \\cdot\\vec {OB}=1-b \\\\ \\omega^T \\cdot\\vec {OA}=-b-1 \\end{array}\\right. \\tag {11}所以： \\begin{split} AP&=\\frac1{|\\omega^T|}\\cdot(1-b+b+1) \\\\ &=\\frac2{|\\omega^T|} \\end{split} \\tag {12}至此，得到了街道的宽度，不难发现街道的宽度只和ω有关，和数据集无关。 那么接下来的任务就是让街道尽可能地宽，即求： max(\\frac2{|\\omega^T|})\\Rightarrow min(|\\omega^T|)为了后续计算方便，优化任务变为： \\begin{split} &min(\\frac12|\\omega^T|^2) \\\\[2ex] &s.t.\\ \\ \\ \\ y_i(\\omega^T \\cdot x + b)-1\\ge0 \\end{split} \\tag {13}4. 朗格朗日根据拉格朗日定理，式（13）可转化为求： L=\\frac12|\\omega^T|^2-\\sum_{i=0}^n\\alpha_i\\big(y_i(\\omega^T\\cdot x_i+b)-1\\big) \\tag {14}的极小值。 对ω和b求偏导，求得极值点为： \\left \\{\\begin{array}{c} w^*=\\sum_{i=1}^n\\alpha_iy_ix_i \\\\[2ex] \\sum_{i=1}^n\\alpha_iy_i=0 \\end{array}\\right. \\tag {15}将式（15）代入（14）得： \\begin{split} L&=\\sum_{i=i}^n\\alpha_i-\\frac 12\\sum_{i=1}^n\\alpha_iy_ix_i\\ \\cdot\\sum_{j=1}^n\\alpha_jy_jx_j \\\\ &=\\sum_{i=i}^n\\alpha_i-\\frac 12\\sum_{i=1}^n\\sum_{j=1}^n\\alpha_i\\alpha_jy_iy_jx_ix_j \\end{split} \\tag {16}其中α为超参数，y为样本观测值，y∈{-1, 1}，所以要求L，关键在于xi·xj的值。 5. 核函数上述的一切都发生在二维平面中，而且假设了存在一条直线可以正确地将正负样本分类。那么如果没有这样一条直线能正确分类样本呢？ 假设这样一种情况，现有如下样本： 图5. 二维不可分很明显，没有一条直线能够将正负样本一份为二，那么，如果这些点在三维空间中呢？如下图： 图6. 三维可分将二维空间中的点映射到三维空间中，假设映射结果为上图。其中负样本全部在红色平面靠里（x坐标小于平面x坐标），正样本全部在蓝色平面朝外（x坐标大于平面坐标），且两个平面平行，这样的话，任何一个平行于这两个平面，且在两者之间的平面，都可以作为分隔正负样本的超平面。 将数据从二维映射到三维，成功地用一个平面正确分割。 那么，只需要找到这个映射关系，向更高维映射，我们就能找到一个超平面，二分类样本。 假设有映射关系： \\Phi(x)=x' \\\\ x=(x_1,x_2...,x_n) \\\\ x'=(x_1',x_2',...,x_m')结合式（16），得： L=\\sum_{i=i}^n\\alpha_i-\\frac 12\\sum_{i=1}^n\\sum_{j=1}^n\\alpha_i\\alpha_jy_iy_j\\Phi(x_i)\\Phi(x_j)观察发现，要求得L，并不需要求出映射关系，只需要知道映射之后的两个点的內积即可。 所以大名鼎鼎的核函数就诞生了，核函数就定义为： kernal(x_i, x_j)=\\Phi(x_i)\\Phi(x_j)于是不需要具体映射关系，只需要给出核函数，就可以求得L。 6. 凸优化给定核函数后，可以将原来的问题转化为一个凸优化问题，具体操作过程在此不进行赘述。 7. 总结 本文谈了自己对SVM模型从无到有的过程的理解，略去了凸优化求解的过程。是因为凸优化问题又是一门学问，想搞清楚并非朝夕之事，所以想把精力集中在堆模型本身的理解上。 今后也会刻意简化优化过程，而注重于对模型、算法本身的理解。 加入了用visio画的图，因为发现python不能画出所以自己想要表达的东西","categories":[{"name":"正儿八经学技术","slug":"正儿八经学技术","permalink":"kasimg.github.io/categories/%E6%AD%A3%E5%84%BF%E5%85%AB%E7%BB%8F%E5%AD%A6%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"kasimg.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"逻辑回归","slug":"logistic-regression","date":"2020-02-26T06:07:59.000Z","updated":"2020-03-19T04:57:36.815Z","comments":true,"path":"2020/02/26/logistic-regression/","link":"","permalink":"kasimg.github.io/2020/02/26/logistic-regression/","excerpt":"逻辑回归常用于分类问题，和线性回归不同的是，前者需要对结果进行分类，而后者是抽象出模型来表征整个数据集。","text":"逻辑回归常用于分类问题，和线性回归不同的是，前者需要对结果进行分类，而后者是抽象出模型来表征整个数据集。 1. 分类问题 图1. 样本点 上图是某班级100个学生两门考试成绩情况，横轴表示第一门考试成绩，纵轴表示第二门，+表示此次考试通过，圆点表示未通过。 现在给定一个学生的成绩，希望预测该学生能否通过此考试。 2. 逻辑回归把通过的样本作为正样本，样本结果为1，未通过的样本视为负样本，样本结果为0。 逻辑回归的任务是训练一个分类器，把学生成绩输入这个分类器，会得到一个0-1之间的数p，p表示这个成绩属于正样本的概率（在这里，就是该同学通过考试的概率）。 不难判断，当p&gt;0.5时，我们认为该学生通过考试，否则认为没有通过。 那么逻辑回归是如何做到这些的呢？ 2.1 sigmoid函数sigmoid函数，表达式如下： S(x) = \\frac 1{1 + e^{-x}} \\tag 1函数图像如下： 图2. sigmoid曲线 当x足够大时，函数值接近1，x足够小时，函数值接近0，而当x=0时，函数值为0.5。 这样的话，无论向函数内输入什么样的值，函数输出总是在0-1之间，符合期望。 之前我有一个疑惑，就是为什么要用sigmoid函数将结果转化为0-1之间的数，现在发现原来是因为正负样本的样本值分别为1和0，如果预测值不转换成0-1之间的数的话，很难计算损失。 2.2 损失函数逻辑回归的损失函数如下： J(\\theta) = \\frac 1m\\sum_{i=1}^m[-y_iln(h_\\theta(x_i)) - (1-y_i)ln(1-h_\\theta(x_i))] \\tag 2向量化之后为： J(\\theta) = -\\frac1m\\big((ln(g(X\\theta))^Ty + (ln(1-g(X\\theta))^T(1-y))\\big) \\tag 3其中： h_\\theta(x) = g(\\theta^Tx) \\tag 4 g(z) = \\frac1{1+e^{-z}} \\tag 5如果采用均方误差，损失函数将不是一个凸函数，所以这里改变了损失函数的形式。 当样本值为1时，损失函数为： J(\\theta) = \\frac 1m\\sum_{i=1}^m[-ln(h_\\theta(x_i))] \\tag 6大约长这个样子 图3. 正样本损失函数 其中横轴为预测值，纵轴为损失。可以看到当预测值x接近0时，损失变得非常大，此时样本值为1，所以这种情况符合预期。 同样的，当样本值为0时，损失图像大概是这样： 图4. 负样本损失函数 当预测值接近1时，损失变得很大，符合预期。 2.3 梯度下降梯度下降之前已经讨论过，这里不做过多赘述，第j个参数的偏导如下： \\frac{\\delta J(\\theta)}{\\delta \\theta_j}=\\frac 1m\\sum_{i=1}^m(h_\\theta(x_i)-y_i)x_{ij} \\tag 7转换为向量形式为： \\frac{\\delta J(\\theta)}{\\delta \\theta_j}=\\frac 1mX^T(g(X\\theta)-y) \\tag 8乍一看这里的（7）式，有些难以理解，那么多ln的式子，求导怎么能得到这个？于是决定 3. 推导偏导数 \\begin{equation}\\begin{split} \\frac{\\delta J(\\theta)}{\\delta \\theta_j}&=-\\frac1m\\sum_{i=1}^m\\big(\\frac{y_i}{h_\\theta(x_i)}\\cdot\\frac\\delta{\\delta\\theta_j}h_\\theta(x_i)-(1-\\frac{1-y_i}{1-h_\\theta(x_i)})\\cdot\\frac\\delta{\\delta\\theta_j}h_\\theta(x_i))\\big) \\\\ &=-\\frac1m\\sum_{i=1}^m\\big(\\frac{y_i}{h_\\theta(x_i)}-\\frac{1-y_i}{1-h_\\theta(x_i)}\\big)\\cdot\\frac\\delta{\\delta\\theta_j}g(\\theta^Tx_i) \\end{split}\\end{equation} \\tag 9而： \\begin{equation}\\begin{split} \\frac\\delta{\\delta\\theta_j}g(\\theta^Tx_i)&=\\frac\\delta{\\delta\\theta_j}\\frac1{1+e^{-\\theta^Tx_i}} \\\\ &=\\frac{e^{-\\theta^Tx_i}}{(1+e^{-\\theta^Tx_i})^2}\\cdot x_{ij} \\end{split}\\end{equation} \\tag 9因为式（5），有： 1-g(\\theta^Tx_i) = \\frac{e^{-\\theta^Tx_i}}{1+e^{-\\theta^Tx_i}} \\tag {10}所以 \\begin{equation}\\begin{split} \\frac\\delta{\\delta\\theta_j}g(\\theta^Tx_i)&=g(\\theta^Tx_i)\\cdot(1-g(\\theta^Tx_i))\\cdot x_{ij}\\\\ &=h_\\theta(x_i)\\cdot(1-h_\\theta(x_i))\\cdot x_{ij} \\end{split}\\end{equation} \\tag{11}那么： \\begin{equation}\\begin{split} \\frac{\\delta J(\\theta)}{\\delta \\theta_j}&=-\\frac1m\\sum_{i=1}^m\\big(\\frac{y_i}{h_\\theta(x_i)}-\\frac{1-y_i}{1-h_\\theta(x_i)}\\big)\\cdot h_\\theta(x_i)\\cdot(1-h_\\theta(x_i))\\cdot x_{ij}\\\\ &=-\\frac1m\\sum_{i=1}^m[y_i-h_\\theta(x_i)]\\cdot x_{ij}\\\\ &=\\frac1m\\sum_{i=1}^m[h_\\theta(x_i)-y_i]\\cdot x_{ij} \\end{split}\\end{equation} \\tag{12}至此，茅塞顿开，浑身舒坦。 4. 结果接下来只要根据梯度下降一步步迭代获取最终模型即可。 下图是分类结果： 图5. 逻辑回归结果 5. 总结 算法实现和画图都是用python做的，这里并没有贴出具体的代码，只谈了对算法的一些认识，因为我觉得代码并不是最重要的，网上一找一大堆，而理解算法本身才是最重要的 markdown的公式编辑真的好用 2020.3.18 补 1. 多分类问题逻辑回归本身是一个二分类问题，如果遇到多分类问题该如何解决？ 首先，逻辑回归可以处理多分类问题，假设有A，B，C三类，有四个分类器L1，L2，L3，L1可以识别出样本属于A或者不属于A，L2可以区分属于B或者不属于B，L3类似。 那么把样本依次经过这三个分类器，就可以得到最终分类。 2.softmax回归上面用多个逻辑回归分类器模拟了多分类的过程，不过多分类问题还可以用一个模型直接解决。 假设有样本： X=(x_1, x_2, ..., x_n) \\\\ Y=(y_1, y_2,...,y_n)其中n为样本数量，m为特征维度。 假设分类的最终结果有k种可能，那么有： \\Theta=(\\theta_1, \\theta_2, ..., \\theta_k)θi（i为1-k的整数）表示每个类别对应的参数。 定义yi被分为第q类的概率为： p(c=q|x_i,\\theta_k)=\\frac{exp(\\theta_q^Tx_i)}{\\sum_{j=1}^kexp(\\theta_j^Tx_i))} \\tag {13}对于给定的yi，令： y_i^k= \\begin {cases} 1,\\ \\ y_i属于第k类 \\\\[2ex] 0,\\ \\ y_i不属于第k类 \\end {cases} \\tag {14}假设yi独立同分布，有关于θ的似然函数： L(\\theta)=\\prod_{i=1}^n (\\prod_{j=1}^k p(c=j|x_i,\\theta_j)^{y_i^j}) \\tag {15}这样的话，后面一个连乘对于每个xi只有一项不为1，所以式（15）也可以直接写成： L(\\theta)=\\prod_{i=1}^np(c=q|x_i,\\theta_q) \\tag {16}因为对于每个yi，其分类已经确定，假设属于第q类，只需要求给定xi后属于第q类的概率即可。 式（16）、式（15）表示从总体样本中取出样本X的概率，这个概率越大，表示从总体样本中抽到这一组数据的概率越大，说明估计的样本分布越接近原始分布，所以需要求L(θ)的最大值。 具体求解过程略…","categories":[{"name":"正儿八经学技术","slug":"正儿八经学技术","permalink":"kasimg.github.io/categories/%E6%AD%A3%E5%84%BF%E5%85%AB%E7%BB%8F%E5%AD%A6%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"kasimg.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"线性回归与梯度下降","slug":"线性回归","date":"2020-02-14T04:22:41.000Z","updated":"2020-03-19T04:59:10.093Z","comments":true,"path":"2020/02/14/线性回归/","link":"","permalink":"kasimg.github.io/2020/02/14/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/","excerpt":"之前由于工作需要重新拾起了机器学习相关的知识，想着一不做二不休，干脆把整个脉络再重新梳理一遍，今天写一点关于梯度下降的理解。 就以线性回归为例来谈谈吧。","text":"之前由于工作需要重新拾起了机器学习相关的知识，想着一不做二不休，干脆把整个脉络再重新梳理一遍，今天写一点关于梯度下降的理解。 就以线性回归为例来谈谈吧。 1. 线性回归1.1 建模假设有学生身高和体重的数据： 学生编号 身高 体重 1 175cm 70kg 2 165cm 50kg 3 185cm 80kg 建立坐标系画出点的分布如下： 图1. 数据点现要预测一个200cm的学生的体重。 我们能大概推算出一个关系：身高越高，体重越重，我们希望用更精确的方式来表达这种关系，这时不难想到中学时学过的解析几何中的一次函数： y=kx+b \\tag 1其中，x表示身高，y表示体重。如果用(1)式表示身高与体重的关系，因为(1)是线性的，所以称这个将模糊的关系精确化的过程叫“线性回归”。 例如将关系总结成如下的直线： 图2. 回归直线 1.2 损失函数现在我们已经得到了一个线性模型，那么如何判断这个模型是不是好呢？ 所谓“好”，就是指和原来的结果接近，最完美的情况下，所有的点都在直线上，当然这并不可能，所以我们要做的事情，就是尽可能地让点接近直线。 假设现在得到的直线是： y=f(x)=\\theta_1 + \\theta_2x \\tag 2于是我们定义第i个点距离直线的距离表示为： D_i=(f(x_i) - y_i)^2 \\tag 3其中xi，yi表示第i个样本的身高值和体重值，Di也可以看做第i个点的样本值与预测值之间的差异程度。 知道了每个点的差异程度，就能知道所有点的差异程度，从而求出平均差异程度： J(\\theta_1,\\theta_2) = \\frac 1{2m}\\sum_{i=1}^m(f(x_i) - y_i)^2 \\tag 4为了方便后续计算，乘上系数1/2。 现在我们的目的很明确，要找到合适的θ，使得差异程度最小。 而这里的差异程度函数，也称作损失函数 经过观察，不难发现这里的损失函数是一个凸函数（具体定义可自行搜索..），这种函数一定有全局最小值，非常适合当做损失函数使用。所以如何构建一个凸函数作为损失函数，是机器学习中的关键。 2. 梯度下降2.1 简单的数学方法求最小值在这个例子里，损失函数是一个二次函数，开口向上，经过简单的数学计算便可以得到最低点。 不过这样计算量非常大，而且仅限于在二元的情况下，如果维度增加，则很难使用此方法解决。 2.2 梯度与偏导梯度，即函数上某一点处使函数值变化最快的方向，转换为代数模式就是导数，导数&gt;0表示增长最快，导数&lt;0表示减少最快。 2.3 关于梯度下降算法的一些思考我看过许多关于梯度下降的文章，大多数都会举“下山”的例子，大概意思是： 人在山顶处，想要下山 沿着负梯度方向，函数值减小最快 选择负梯度方向迈一小步 不停重复上面一个步骤，直到到达山底 “迈出一小步”用公式可以描述为： \\begin{equation}\\begin{split} \\theta_1 : = \\theta_1 - \\alpha\\frac {dJ(\\theta_1, \\theta_2)}{d\\theta_1} \\\\ \\theta_2 : = \\theta_2 - \\alpha\\frac {dJ(\\theta_1, \\theta_2)}{d\\theta_2} \\end{split}\\end{equation} \\tag 5其中α称为学习率，用来控制这里步长的大小。 到这里，我产生了一个疑问，如何解释α与偏导相乘的意义呢？如果因为负梯度方向减少最快，所以沿着此方向迈步，那么“下山”的目标就是： 以最快的速度下山 而如果在一个非直线的函数上，“沿着负梯度方向迈一步”，那么这一步一定会落到函数外侧，那么这一步又有什么意义呢？体现在上面的式子中，-α与偏导相乘表示在负梯度方向跨出一步，而θ只会在横轴或者纵轴方向改变，前后两项方向不一样，如何进行加法运算呢？ 所以我百思不得其解，什么叫“沿着负梯度方向迈一步”，还有，下山的目标真的是“以最快的速度下山吗？” 最快的速度，无非是一步直接跨到山底，但是因为α的存在，真的可能吗？ 所以我想，可能我们不是想“以最快的速度下山”，而是“以最有效的方式下山” 那么怎样才能“最有效”呢？抛开“负梯度方向下函数值减少最快”，去观察凸函数的梯度本身，发现： 越接近函数底部，梯度绝对值越小，变化越平缓，体现在上式中，一开始梯度模比较大，所以α与偏导相乘的值比较大，跨出的步子比较大；而越来越接近底部时，梯度模变小，此时跨出的步子就越小；非常切合实际，越到后面，步子越是要小，否则容易跨过头 如果不小心步子太大，容易直接越过底部，跨到另一边山坡上。而此时导数值符号变化，导致了原来的“向前跨”变成了“向后跨”，这样不至于一直沿着错误的方向跨步。 个人认为，上面的两条原因才是让α与偏导数相乘的原因，而并非因为“负梯度方向下函数值减少最快”。这样的话，α才能真正表示“步长”，而导数，只是从函数中发现的，切合此时情景的参数。 2.4 梯度下降算法经过不断迭代θ值后，使得损失函数越来越小，小到某一范围时，停止算法，变完成了梯度下降算法。 至于何时停止，这篇文章就先不讨论啦.. 3. 回归结果 图3. 回归结果 4. 总结 写出了自己对于梯度下降法的一些理解 忽略了一些细节，后续考虑写一些细节方面的东西 2020.3.16 补 之前直接使用了最小二乘法，而没有深究为何使用它，这里便从极大似然估计角度来谈谈这里使用最小二乘的意义。1. 构建正态分布见下式： y_i = \\theta^Tx_i + \\xi_i \\tag 6其中θTxi为预测值，yi为样本观测值，ξi为误差。 假设样本是独立同分布的，根据中心极限定理，当样本总体数量足够大时，ξ服从正态分布（μ，δ2），总可以调整θ，将常数项对应的θ0减去μ，使得ξ服从（0，δ2）的正态分布。 2. 似然函数此时ξi的概率密度函数为： P(\\xi_i)=\\frac 1{\\sqrt{2\\pi}\\sigma}exp\\left(-\\frac {(\\xi_i)^2}{2\\sigma^2}\\right) \\tag 7将式（6）带入式（7），得： P(y_i|x_i;\\theta) = \\frac 1{\\sqrt{2\\pi}\\sigma}exp\\left(-\\frac {(y_i-\\theta^Tx_i)^2}{2\\sigma^2}\\right) \\tag 8因为样本独立同分布，于是有： \\begin{split} L(\\theta) &= \\prod_{i=1}^mP(y_i|x_i;\\theta) \\\\ &= \\prod_{i=1}^m\\frac 1{\\sqrt{2\\pi}\\sigma}exp\\left(-\\frac {(y_i-\\theta^Tx_i)^2}{2\\sigma^2}\\right) \\end{split} \\tag 9L为似然函数，表示这组样本出现的联合概率。而如今的任务是找到一组θ，使得联合概率最大，这就是所谓的极大似然估计。 3. 最小二乘根据式（9），有： \\begin{split} lnL(\\theta) &= \\sum_{i=1}^mln\\frac 1{\\sqrt{2\\pi}\\sigma}exp\\left(-\\frac {(y_i-\\theta^Tx_i)^2}{2\\sigma^2}\\right) \\\\ &=\\sum_{i=1}^m\\left(ln \\frac1{\\sqrt{2\\pi}\\sigma} -\\frac {(y_i-\\theta^Tx_i)^2}{2\\sigma^2}\\right) \\\\ &=mln \\frac1{\\sqrt{2\\pi}\\sigma}-\\frac1{2\\sigma^2}\\sum_{i=1}^m(y_i-\\theta^Tx_i)^2 \\end{split} \\tag {10}令： J(\\theta) = \\frac 12\\sum_{i=1}^m\\big(h_\\theta(x_i)-y_i\\big) \\tag {11}上式便是目标函数，不难得到： 当J(θ)最小时，似然函数取得最大值 所以最小化J(θ)就成了最终的目标 2020.3.17 补所谓线性回归，指的是对于参数而言是线性的，而非对样本而言。","categories":[{"name":"正儿八经学技术","slug":"正儿八经学技术","permalink":"kasimg.github.io/categories/%E6%AD%A3%E5%84%BF%E5%85%AB%E7%BB%8F%E5%AD%A6%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"kasimg.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"对于PCA算法的个人理解","slug":"PCA","date":"2020-02-11T03:00:19.000Z","updated":"2020-03-19T05:02:28.155Z","comments":true,"path":"2020/02/11/PCA/","link":"","permalink":"kasimg.github.io/2020/02/11/PCA/","excerpt":"由于项目需要，复习了一下读书的时候学过的PCA降维算法，由于太久没用的缘故，忘记了不少。所谓温故而知新，遂以此文章为引，阐述自己对此算法的理解，记录下新的感悟。","text":"由于项目需要，复习了一下读书的时候学过的PCA降维算法，由于太久没用的缘故，忘记了不少。所谓温故而知新，遂以此文章为引，阐述自己对此算法的理解，记录下新的感悟。 1. 降维的意义1.1 遇到问题考虑如下一组数据： 学生编号 性别 头发长度 身高 腿长 体重 衣服尺码 1 男 4cm 175cm 90cm 70kg xl 2 女 40cm 165cm 80cm 50kg m 3 男 3cm 185cm 95cm 80kg xxl 数据一共6个维度，性别、头发长度、身高、腿长、体重、衣服尺码。 经过观察不难看出： 头发的长度和性别息息相关，女生的头发大多比男性的头发长 腿长和身高有关，一般来说，身高越高，腿越长 衣服尺码随身高、体重增加而增大 由于数据各个维度高度相关，导致了有些信息是几乎无用的，比如说衣服尺码，通过身高和体重就能推测出衣服尺码，所以衣服尺码这一信息可有可无；同样的，腿长可以根据身高推测出，这一信息也可以省略。 1.2 数据降维去除数据中的无效维度，只留下主要的维度，或者说主成分。这个过程过程，就可以称之为数据降维。换句话说，我们希望数据各个维度之间是没有关系的，相互独立的。 数据降维的目的就是使数据更加简洁明了。 2. 数学基础略 3. 建模3.1 确定优化目标上面提到，数据降维相当于把一个高维度的向量，映射到一个低维度向量空间中。 假设有二维空间中有5个点： （1，1） （1，3） （2，3） （4，4） （2，4） 表示为矩阵形式： \\begin{bmatrix} 1 & 1 & 2 & 4 & 2 \\\\ 1 & 3 & 3 & 4 & 4 \\\\ \\end{bmatrix} \\tag{1}为了后续表示和计算方便，进行中心化处理，将数据每个维度的的值减去该维度的均值，处理后得到以下结果： \\begin{bmatrix} -1 & -1 & 0 & 2 & 0 \\\\ -2 & 0 & 0 & 1 & 1 \\\\ \\end{bmatrix} \\tag{2}5个点在二维向量空间中表示如下： 图1. 向量空间中的点 现在目的是将这些数据映射到一维空间中去，即将图中的这些点映射到一条直线上。 那么问题来了，如何映射才能保证信息最大程度地保留呢？ 假设映射到横轴上，那么横坐标为-1的两个点会合二为一，直接导致了一个点的信息丢失，横坐标为0的点也是如此。 同样的，假设映射到纵轴上，那么也有两个点的信息会丢失。 既然映射之后点的距离太近会导致丢失，那么让他们的距离越远越好。或者说，让他们越分散越好。 3.2 方差在数学中，方差被用来描述数据的离散程度，方差越大，离散程度越高。 所以上面的目标可以转化成，让映射之后的点的方差尽可能地大。 方差的计算公式为： Var(a)=\\frac 1{m-1} \\sum_{r=1}^m{(a_i - \\mu)}^2 \\tag{3}其中a表示数据集，m表示数据个数，ai表示其中某一个数据，μ表示平均值。 由于之前进行过中心化处理，所以可以简化为： Var(a)=\\frac 1{m-1} \\sum_{r=1}^m{a_i}^2 \\tag{4}对于二维向一维映射问题来说，只要找到一个方向，或者找到一条直线，使得映射之后的数据方差最大，那么就得到了最好的映射关系。 3.3 协方差刚才是二维向一维投影，映射之后得到的是一维数据，方差计算较为方便，如果维数更高该怎么办？ 回到一开始的学生信息问题上，我们总是希望数据的各个维度之间没有关系，那么如何衡量数据维度之间的关系呢？ 答案是协方差。假设有数据集X： X= \\begin{bmatrix} a_1 & a_2 & ... & a_m \\\\ b_1 & b_2 & ... & b_m \\\\ \\end{bmatrix} \\tag{5}那么维度a，b的协方差为： Cov(a,b)=\\frac 1{m-1} \\sum_{r=1}^m{(a_i - \\mu_a)}{(b_i - \\mu_b)} \\tag{6}由于进行过中心化，所以有： Cov(a,b)=\\frac 1{m-1} \\sum_{r=1}^m{a_i}{b_i} \\tag{7} 协方差 &gt; 0，表示呈正相关关系 协方差 &lt; 0，表示呈负相关关系 协方差 = 0，表示不相关 3.4 协方差矩阵将协方差写成矩阵形式： \\begin{bmatrix} \\frac 1{m-1} \\sum_{r=1}^m{a_i}^2 & \\frac 1{m-1} \\sum_{r=1}^m{a_ib_i} \\\\ \\frac 1{m-1} \\sum_{r=1}^m{a_ib_i} & \\frac 1{m-1} \\sum_{r=1}^m{b_i}^2 \\\\ \\end{bmatrix} = \\frac 1{m-1}XX^T =C \\tag{8}这里的C就表示X的协方差矩阵，对角线上的元素表示维度的方差，其余元素表示两个维度之间的协方差。 我们希望C经过某种变换之后，只有主对角线上的元素不为0，其他元素都为0，这样的话表示两个维度的数据不相关，能够表示最多的信息量，而这个变化就是要寻找的映射关系。 也就是说，通过变换P，使得C成为一个对角矩阵。 现在假设数据集X的协方差矩阵为C，Y是映射后的数据，P是映射变换矩阵，D是Y的协方差矩阵，那么有： \\begin{equation}\\begin{split} Y&=PX \\\\ D&=\\frac 1{m-1}YY^T \\\\ &=\\frac 1{m-1}(PX)(PX)^T \\\\ &=\\frac 1{m-1}PXX^TP^T \\\\ &=P(\\frac 1{m-1}XX^T)P^T \\\\ &=PCP^T \\end{split}\\end{equation} \\tag{9}所以，此时的目标是： 找到矩阵P，使得D成为对角矩阵 3.6 对角化协方差矩阵伟大的数学先驱们已经为我们总结了许多实对称矩阵的性质，其中有一条： n阶实对称矩阵A必可相似对角化，且相似对角阵上的元素即为矩阵本身特征值。 所以我们一定能找到一个矩阵P，使得式（9）成立 3.7 PCA变换现在，我们得到了映射变换矩阵P，和对角矩阵D（对角线元素自上而下递减），接下来需要选择需要映射到的空间的维度。 假设要映射到K维空间，那么就取P的前k行，此时映射后的数据集Y为： Y_{k×n}=P_{k×m}X_{m×n}至此，完成了PCA算法的所有步骤，成功将数据降维。 4. 总结 文章并未涉及如何选择维度的问题，下次讨论… 2020.3.31 补 之前从数据维度两两不相关角度阐述了对PCA的理解，今天尝试从另外一个角度来解释它。 1. 方差作为目标函数延续之前的3.2，方差越大，说明数据越离散，那么映射效果越好，那么不妨将投影之后的方差作为目标函数。 假设映射关系为P，原数据为x，E为单位矩阵，那么方差为： Var(X) = (XP - E)^T \\cdot (XP - E) \\tag {10}不妨假设已经做过中心化处理，即E=0，再限定u的尺度为1，防止其有无数种取值可能，有： \\begin {split} &Var(X) =P^TX^TXP \\\\ &s.t. \\ \\ \\ \\ P^TP=1 \\end {split} \\tag {11}此时式（11）变成一个带限制条件的求极值问题。 根据拉格朗日定理不难得到，在： X^TXP=\\lambda P \\tag {12}时，目标函数取得极值。入表示拉格朗日参数。 显然XTX为对称矩阵，那么式（12）就可以理解为求方阵XTX的特征值和特征向量，n阶对称矩阵有n个不同的特征值入i，将XTX和对应的特征向量Pi相乘，得到数据在Pi维度的投影，若此时结束，那么就获得了一个一维投影；而如果想要更多维度的数据，那么就要找到更多的Pi，将数据投射到更多的维度。因为对称矩阵的特征向量是两两正交的，所以每个维度之间是不相关的，能够保证反映最大的信息量。 所以，将求得的特征值从大到小排列，选取前k个最大的，就完成了将数据维度降低到k维的效果。","categories":[{"name":"正儿八经学技术","slug":"正儿八经学技术","permalink":"kasimg.github.io/categories/%E6%AD%A3%E5%84%BF%E5%85%AB%E7%BB%8F%E5%AD%A6%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"kasimg.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"react组件中的key属性","slug":"react-key","date":"2020-01-06T04:43:22.000Z","updated":"2020-01-06T05:42:02.666Z","comments":true,"path":"2020/01/06/react-key/","link":"","permalink":"kasimg.github.io/2020/01/06/react-key/","excerpt":"","text":"一直以来，我对react（vue）组件中的key属性并不在意，加上它的原因只不过是不想看到讨厌的warning。 直到昨天，我才发现key的意义所在，以及为什么不推荐用index作为key的值。 1. 遇难题简单介绍一下背景： 一组组件，可以添加或者删除 组件信息从数据库中获取，按时间倒序排列 每个组件中需要和数据库交互，获取额外信息 当时用的key值就是数组中的index，导致组件在增加删除后刷新时出现了问题，过程如下： 原有三个组件，id为：a、b、c，组件中通过数据库获取的数据分别为x、y、z 添加数据d，此时数据库中没有关于d的数据，d组件获取不到数据 重新渲染后，有a、b、c、d四个组件，对应的数据为x、y、z、z 那么问题来了，d应该什么数据也没有，为什么会有数据z呢？ 2. 排查第一时间想到追踪每个组件的数据读取操作，当时数据读取操作写在componentDidMount中，于是在componentDidMount中打印当前组件的id，结果如下： 增加d之前，打印出a、b、c 增加d之后，打印出c 增加d之后，只打印了一次，且打印出了c，说明只有一个c控件被重新渲染。那么为什么只有c被重新渲染？ 结合渲染组件时使用的map操作，突然发现了key这个属性可能是关键所在。 3. 解惑找到了方向，立刻着手分析。 当时的key属性的值是index。增加d之前，a、b、c分别对应0、1、2 增加d之后，由于读取时按时间倒序排列，所以a、b、c、d分别对应1、2、3、0，组件实际的顺序为：d、a、b、c 这么一来，问题就差不多解决了，增加d之后，d、a、b都没有重新渲染，只有c重新渲染了，因为前三个组件的key值和原来一样，所以并没有重新渲染，第四个组件c由于key值为3，原来不存在这样一个key，所以c重新渲染了，并获取了数据z 由于第三个组件没有重新渲染，所以保留着上一次渲染的内容，即z，所以出现了数据z并不是因为数据读取错误，而是因为第三个组件压根没有重新渲染。 问题迎刃而解。 4. 修复鉴于这个问题，把key属性的值改为了数据库记录的唯一标识，问题解决。 5. 总结 这种遇到问题，解决问题，加深对原理的认识的方式很不错，能让人印象深刻 基础还不是很牢，前路漫漫","categories":[{"name":"正儿八经学技术","slug":"正儿八经学技术","permalink":"kasimg.github.io/categories/%E6%AD%A3%E5%84%BF%E5%85%AB%E7%BB%8F%E5%AD%A6%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"react","slug":"react","permalink":"kasimg.github.io/tags/react/"}]},{"title":"坑坑洼洼的react native之二：node版本","slug":"node-version","date":"2019-12-24T08:20:28.000Z","updated":"2019-12-24T08:26:09.664Z","comments":true,"path":"2019/12/24/node-version/","link":"","permalink":"kasimg.github.io/2019/12/24/node-version/","excerpt":"","text":"今天帮同事搭react native的环境，遇到了之前没有遇到的坑。 1. javajava是先决条件，这点很容易被忽略，因为官方文档中关于java的部分被一笔带过。 2. node版本如果node版本不对的话，最后运行项目时node窗口会闪退，导致最后报错，报错信息也很迷，所以这点也需要注意","categories":[],"tags":[{"name":"react native","slug":"react-native","permalink":"kasimg.github.io/tags/react-native/"}]},{"title":"定时器的运用","slug":"timer","date":"2019-12-11T01:05:34.000Z","updated":"2019-12-11T02:08:16.990Z","comments":true,"path":"2019/12/11/timer/","link":"","permalink":"kasimg.github.io/2019/12/11/timer/","excerpt":"","text":"前一阵子完成了一个有关定时器的任务：在RN中根据时序画图。下面就说说这个还算是有些波折的过程。 1. 背景需求整理如下： 有若干个点存放在sqlite中，点的信息有xy坐标、ts时间戳（ts递增）等 取出这些点，按照时间顺序画出来 2. 第一次尝试刚接到任务的时候第一反应是： 取出所有的点 对于每个点，创建一个setTimeout 取第一个点的ts作为基准时间点 setTimeout的延时长度为对应点ts与基准时间的差值 这样一来，每个点都能在规定的时间被画出来了，看起来没有啥问题，而且实现非常简单。 不过事实并非如此，遇到了如下的问题： 假设有1W个点，那么同时会有几千个定时器存在，占用超多内存，卡爆 由于ts是递增的，到后面定时器的延时时间会非常长，导致报出警告 其实第一个问题就足以让我换一种方法了。 3. 前进一小步既然定时器太多，那么如何减少定时器的数量呢？不难想到用setInterval代替setTimeout，不过问题是，前者只能使用固定的时间间隔，而实际ts间隔是不同的，那么如何解决这个问题？想到了如下折中的办法： 设定间隔时间阈值t(t &gt; 0)，任意两个ts之差的绝对值小于t的点归为一类 同一类的点视作间隔相等，间隔时间为t 每一类对应一个setInterval 第一个setinterval结束后，另一个setinterval开始 这样有两个问题： 1. t过大，大部分点都属于同一类，时间间隔相同，无法复现原有情景 2. t过小，种类过多，导致定时器依然很多 经过多次改变t，找到了比较好的中间值，不过结果仍然差强人意，而且实现较为困难。 4. 停下思考对于有些强迫症的我来说，上面的结果是没有办法接受的，思考了一阵，觉得有两条路： 回归setTimeout，使用两个相邻点的ts之差作为延时量，代替原来的与基础ts的差，且前一个setTimeout完成后才进行下一个setTimeout 想别的办法。 考虑到第一条路实现非常繁琐，需要各种flag判断，再加上经过上面的过程之后，想要找一个精简一点的方法，遂考虑一切推翻重来。 5. 转折正在我思考该如何重来的时候，正在睡午觉的同事的闹钟响了，同事缓缓抬起了头，睁开惺忪的睡眼，站起来去洗手间…等等！闹钟，闹钟，闹钟…!对了，闹钟！ 我可以只创建一个定时器，定时器一直向前走，到了某个时间点唤醒对应的点。 之前我所有的想法都是将点和计时器紧密结合起来，每个点或者几个点对应一个计时器。没有考虑过奖定时器和点分离开来。 6. 大跃进改进后方法如下： 创建一个setinterval，步距设为s 取第一个点的ts为基准ts，取名baseTS 从第一个点开始判断，判断点的ts与baseTS的差值的绝对值是否小于s 如果小于s，那么就画出这个点 否则，将baseTS + s 重复3-5，直到将所有点都画出 这样可以保证每个点都能被画出来，而且步距s的可取范围变得更大。 最终达到了一个令人满意的结果，而且代码量也很少 7. 总结优化无止境，精益求精。","categories":[{"name":"正儿八经学技术","slug":"正儿八经学技术","permalink":"kasimg.github.io/categories/%E6%AD%A3%E5%84%BF%E5%85%AB%E7%BB%8F%E5%AD%A6%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"react native","slug":"react-native","permalink":"kasimg.github.io/tags/react-native/"},{"name":"javaScript","slug":"javaScript","permalink":"kasimg.github.io/tags/javaScript/"}]},{"title":"一些优化","slug":"optimization","date":"2019-12-05T02:51:23.000Z","updated":"2019-12-06T01:29:21.388Z","comments":true,"path":"2019/12/05/optimization/","link":"","permalink":"kasimg.github.io/2019/12/05/optimization/","excerpt":"","text":"sql优化 数据准备优化、RN和android之间通信速度优化 今天review了一下代码，发现以及改正了一些效率低下的地方。 1. RN和android的数据通信因为需求需要，之前写了一个RN和android通信的模块，实现了从android端向RN端传送N条数据，具体实现官网上都有教程，且我并没有遇到什么问题，所以不在此赘述。 当时的实现思路是： 每次传送一条数据 传送N次 传送了1W条数据，每条数据是长度约20的字符串，一共耗时36s，可能由于缺乏对这方面速度概念的理解，当时并没有觉得有什么大问题，直到向BOSS汇报的时候… 于是着手改进。 整理了一下思路，速度主要分为两个部分： 传输操作消耗的基础时间 由于携带数据消耗的额外时间 每次传输的数据量越大，消耗的时间就越长，但是传输次数就减少；而减少每次的传输量，能够加速单次传送，不过传送次数又增加。 想要找到最优解的话，就需要知道每次消耗的时间与数据大小的具体关系。 设计了简单的测试，步骤如下： 一次传送所有数据 结果传送1W条数据只用了1s，可以得到基础结论：单次传输时间成本很高，需要尽量减少传送次数。 为了提高效率，这次就采用了这个较优解，至于最优解，在闲余时间会深入研究。 2. RN和sqlite通信和上面的例子非常相似，只不过这次是关于数据库的操作。 背景是从sqlite中读取1W条数据，之前的步骤是： 读1条数据 读1W次 耗时20s，更改之后的步骤： 一次读所有数据 耗时1s。 3. 总结 RN和android之间的通信是跨语言的，成本比较高，需要减少通信次数 数据库的读写操作成本也很高，需要减少与数据库的交互次数 数据量本身造成的时间成本相较于上面两点，并不高","categories":[{"name":"正儿八经学技术","slug":"正儿八经学技术","permalink":"kasimg.github.io/categories/%E6%AD%A3%E5%84%BF%E5%85%AB%E7%BB%8F%E5%AD%A6%E6%8A%80%E6%9C%AF/"},{"name":"代码review","slug":"正儿八经学技术/代码review","permalink":"kasimg.github.io/categories/%E6%AD%A3%E5%84%BF%E5%85%AB%E7%BB%8F%E5%AD%A6%E6%8A%80%E6%9C%AF/%E4%BB%A3%E7%A0%81review/"}],"tags":[{"name":"react-native","slug":"react-native","permalink":"kasimg.github.io/tags/react-native/"}]},{"title":"折腾hexo之三：配置与命令","slug":"hexo-config","date":"2019-12-04T04:57:02.000Z","updated":"2019-12-03T05:58:44.618Z","comments":true,"path":"2019/12/04/hexo-config/","link":"","permalink":"kasimg.github.io/2019/12/04/hexo-config/","excerpt":"","text":"这次研究了一下hexo的各种命令和_config.yml配置文件。 1. 命令仅对于常用的命令谈谈自己的理解。 初始化命令 1hexo init 此命令用户在当前目录下生成一个hexo项目 生成部署文件命令 1hexo g 此命令用于在目录下生成public文件夹，而public文件夹中的内容就是之后要部署到服务器上的内容 清除命令 1hexo clean 此命令用于将public文件夹删除，一般在hexo g命令之前使用此命令，防止文件冗余（直接hexo g的话，可能有一些不需要的之前生成的文件残留下来） 开启本地服务命令 1hexo s 部署命令 1hexo d 此命令用于将public文件夹部署到指定服务器上 2. 根目录下的_config.yml配置文件","categories":[{"name":"正儿八经学技术","slug":"正儿八经学技术","permalink":"kasimg.github.io/categories/%E6%AD%A3%E5%84%BF%E5%85%AB%E7%BB%8F%E5%AD%A6%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"kasimg.github.io/tags/hexo/"}]},{"title":"坑坑洼洼的react native之一：和sqlite不得不说的故事","slug":"sqlite-issues","date":"2019-12-04T04:54:35.000Z","updated":"2019-12-04T11:09:34.275Z","comments":true,"path":"2019/12/04/sqlite-issues/","link":"","permalink":"kasimg.github.io/2019/12/04/sqlite-issues/","excerpt":"","text":"配置问题 多次加载问题 画板优化 因为业务需要，最近开始接触react native，不学不知道，一学才发现，这一定是一条坑洼大道。因为第一步配置就折腾地我死去活来，下面就用文字来发泄我的郁闷之情。 1. react native配置由于用的windows本，所以是按照win+android来配置的，简单说下流程。 nodejs，java环境配置，react-native-cli脚手架安装，不多赘述只需要注意node和npm的权限问题即可 Android环境配置，这里需要认真看官网给出的教程，不然很容易漏装一些乱七八糟的东西，其次注意环境变量的名称和目录即可 创建模拟器，选对安卓版本即可 创建新项目，一行命令搞定。 编译运行项目，一行命令搞定，这里有些点需要注意 要在项目根目录下（有ios和android文件夹）执行命令，不能进入某一个文件夹下 有时编译会不通过，并报“Failed to install the app. Make sure you have the Android development environment set up”的错误，具体原因我不知道，解决方案如下： 把node_modules文件夹删掉，重新npm install，再执行编译命令 干掉整个项目，新建一个 这样一来，react native的基本环境就搭建好了，到这里为止还是比较简单的。 2. react native 下的sqlite插件sqlite对于原生有很好的支持，而RN封装了这些方法，将接口提供给js使用，为了（lao）提升（ban）代码（yao）的复（qiu）用性（de），我毅然决定在RN中写好对数据库的操作，在ios和android两端只要进行配置即可。 谁知道，前路如此坎坷，诸君请听我细细道来。 2.1 配置仅以android端为例，过程参照 https://github.com/andpor/react-native-sqlite-storage 整理如下： npm安装插件，这一步没有什么问题 更改android配置文件settings.gradle。这是卡我卡的最久的一步，按照说明更改配置后，android studio中同步时一直报错，如下： 墙里墙外访问过无数网站，试验过千百种方法……最终发现了问题所在。New File(…)中的路径错了，少了一个/platforms（可是原来错误的路径下也有android文件夹啊！！我特地检查过的！！），下面是正确路径：： 所以问题来了，为啥github和npm上的教程步骤都不对？？？为什么？why？なぜ？ 没人能够回答我… 配置build.gradle文件，这里没有问题 将插件添加到package中，以供js使用。大坑2号出现了！卡我第二久，按照配置配置完之后，美滋滋的开始编译，以为终于雨过天晴，可是迎来的只有冷冷的冰雨，还在脸上胡乱地拍。又报错了….啊啊啊啊！！错误如下： 大致意思是MainApplication模块被创建了两次…于是查看了一下日志，发现启动时有这么一段话： 大致意思是RN会自动加载依赖，可是react-native-sqlite-storage这个依赖被手动加载了，这也和之前的错误对应上了，那么就按照他的解决方案来吧，输入了下面的命令： 1react-native unlink react-native-sqlite-storage 果然还是不行，呵呵，我转过头，外面的天空好蓝，偶尔有几只鸟飞过。 所以，到底是哪里重复创建了？？？ 上了个厕所冷静了一下，重新捋了一遍，发现第三步中，显式地将SQLitePluginPackage加入了packages中，难道这里不需要加进去？管他三七二十一，先试试。 可！以！了！！！ 瞬间浑身舒爽… 等等，那文档里写的啥？？？为啥让我多此一举？？难道是文档没有及时更新吗？？ ……………………. 至此，sqlite插件成功配置完成，额不对，完成了一半，因为还有ios端… 路漫漫，坑洼不断，不过我还是相信总有花明柳暗。 3. 总结总结一下今天的配置过程，很痛苦，不过也学到了很多东西，也培养了屡败屡战的不屈精神，总体来说收获颇丰。 更重要的一点，以后遇到报错一定要截图！！否则后期为了文章里的图还要重现一下bug…","categories":[{"name":"正儿八经学技术","slug":"正儿八经学技术","permalink":"kasimg.github.io/categories/%E6%AD%A3%E5%84%BF%E5%85%AB%E7%BB%8F%E5%AD%A6%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"react native","slug":"react-native","permalink":"kasimg.github.io/tags/react-native/"},{"name":"sqlite","slug":"sqlite","permalink":"kasimg.github.io/tags/sqlite/"}]},{"title":"折腾hexo之二：更换主题","slug":"beautify","date":"2019-12-03T01:22:11.000Z","updated":"2019-12-03T04:59:21.505Z","comments":true,"path":"2019/12/03/beautify/","link":"","permalink":"kasimg.github.io/2019/12/03/beautify/","excerpt":"作为一个颜控，hexo的默认主题并不能满足我，于是我开启了对博客颜值的追求之旅。","text":"作为一个颜控，hexo的默认主题并不能满足我，于是我开启了对博客颜值的追求之旅。 1. 弱水三千取一瓢打开官网主题页面，200+主题任君挑选。经过一番挑选，最终选择了meterial-x这款主题，无论色调还是风格都是我喜欢的类型，下面是主题截图： (这里不得不提的是，在图片加载过程中遇到了许多问题，会在下一小结详细说明) 还有实例网址： https://xaoxuu.com/ &lt;/br&gt; 2. 插叙：图片加载问题这个问题是在写这篇blog时遇到的，困扰了我一阵子，希望我的解决方案可以帮到以后阅读这篇文章的人。 2.1 原生markdown图片语法问题原来的语法能在文档编辑时正常显示，不过部署后无法显示。于是上网搜集解决方案，先进行了以下操作： 将_config.yml文件中的post_asset_folder配置项设置为true 安装hexo-asset-image插件 1npm install hexo-asset-image --save 此时使用 1npx hexo n post &quot;balabala&quot; 新建文档时，会发现不仅创建了文档本身，还创建了和文档同名的文件夹。将图片放入此文件夹中，对应文档中使用相对路径应该就可以完成图片的显示。 可是并没有。 无奈之下，寻求别的出路。 2.2 官网提供的语法也不行？去hexo官网，发现有专门针对资源的加载语法： 1&#123;% asset_img image.jpg description %&#125; &#x2F;&#x2F; asset_img表示图片资源，image.jpg表示文件夹下图片 的名称，description表示图片描述 立马试了一下，发现还是不行 苦恼之下，打开了chrome开发者工具，试图寻找一丝线索。 结果发现网页中图片的地址解析成了/.com//2019/12/03/文件夹/图片.jpg，问题就出在这里，为啥解析成了这奇怪的东西？难道官网给出的方法也不靠谱，解析出了问题？ 2.3 hexo-asset-image插件bug“只不过是从头再来…”，脑海中想起了刘欢的声音。 于是我把一切推翻重来，按照官网的步骤重新搭建了环境，这次我发现一个奇怪的事情：官网上根本没有提到关于hexo-asset-image插件的事情，而且有这么一段话： 在Hexo 2时代，社区创建了很多插件来解决这个问题。但是，随着Hexo 3 的发布，许多新的标签插件被加入到了核心代码中。这使得你可以更简单地在文章中引用你的资源。 我查看了一下package.json文件，发现hexo的版本是4.0，是3.0之后的版本，所以我用2.0版本的方法试图解决4.0版本的问题，所以出了问题。 所以我这次没有安装hexo-asset-image插件，最后成功在网页上渲染出了图片。 PS: 后来在网上收集资料，确实是hexo-asset-image插件出了问题 最后总结一句话，使用步骤尽量参考官方文档，网上有些博客提供的方法确实不靠谱… &lt;/br&gt; 3. 安装主题安装步骤如下： 下载主题文件，放到项目跟目录下的themes文件夹 在_config.yml文件中找到配置项theme，设置为主题文件夹的名称 重新生成public文件夹： 1npx hexo clean2npx hexo g 这里clean操作可以清除根目录下的public文件夹中的内容，g表示generation，可以生成public文件夹。（public文件夹中的内容就是部署到服务器上的内容） 启动服务： 1npx hexo s 这时就可以看到主题已经应用上了。 &lt;/br&gt; 4. 总结今天更换了喜欢的主题，并且解决了图片无法正常显示的问题，下次准备研究一下_config.yml文件中的各个配置项。","categories":[{"name":"花里胡哨瞎折腾","slug":"花里胡哨瞎折腾","permalink":"kasimg.github.io/categories/%E8%8A%B1%E9%87%8C%E8%83%A1%E5%93%A8%E7%9E%8E%E6%8A%98%E8%85%BE/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"kasimg.github.io/tags/hexo/"}]},{"title":"折腾hexo之一：遇见hexo","slug":"遇见hexo","date":"2019-12-02T12:58:37.000Z","updated":"2019-12-03T04:59:22.944Z","comments":true,"path":"2019/12/02/遇见hexo/","link":"","permalink":"kasimg.github.io/2019/12/02/%E9%81%87%E8%A7%81hexo/","excerpt":"一直想搭建一个属于自己的博客，今天终于把这个想法付诸实践，整理一下流程以做纪念，也作为本博客的第一篇文章。","text":"一直想搭建一个属于自己的博客，今天终于把这个想法付诸实践，整理一下流程以做纪念，也作为本博客的第一篇文章。 1. 选型通过网上搜集资料，了解到了3种搭建博客的方式： hexo + github，基于js WordPress，基于php hugo，基于go 考虑到自己对js比较熟悉，再加上能免费把博客挂在github，毫不犹豫选择了hexo。 2. 环境搭建大方向确定之后，便开始着手环境的搭建，流程整理如下： 安装nodejs，由于hexo基于node，所以node是必须的。值得注意的是，window下安装node时可能报2503错误，这是权限导致的问题，解决方法是：使用管理员命令行，将安装文件拖入命令行回车即可。另外，使用npm时如果需要全局安装，那么也需要管理员权限 安装hexo-cli，这是hexo的脚手架工具，用于创建hexo项目，安装命令如下： 1npm install hexo-cli -S 这里我采用了局部安装的方法，因为嫌权限切换太麻烦..而且把hexo-cli包存到package.json文件中，方便之后npm install一条命令安装所有包。如果是全局安装，那么需要管理员权限，命令如下： 1npm install -g hexo-cli 安装hexo，用于对项目进行各种操作 1npm install hexo -S 同样，我选择了局部安装 新建文件夹，用作hexo项目的根目录，在目录下创建hexo环境 1npx hexo init 这里由于我没有加入环境变量，所以加上了npx（具体见官网） 使用以下命令创建本地服务器，预览博客效果 1npx hexo s 通过终端中给出的地址访问博客。至此，环境搭建完成。 创建github仓库，仓库名称必须为：username.github.io，其中username更换成github的id 建立hexo和仓库的连接。打开hexo项目目录下的_config.yml，拉到最低，看见如下代码段： 1# Deployment2## Docs: https://hexo.io/docs/deployment.html3deploy:4 type: git5 repo: 仓库地址6 brach: master 其中有些字段后来加上，type表示部署的方式，这里填上git；repo是repository的缩写，表示仓库，这里填上自己在上一步建立的仓库地址即可；branch默认就是master，这里显式标出，更加清晰 将项目部署到github。一条命令即可： 1npx hexo d 期间会提示输入github账号密码，正常输入即可。至此，部署完成，可以访问username.github.io查看博客。 3. 总结今天简单的搭建了环境，不过仅仅处于“能运行”的状态，距离博客成型还有一段距离。明天试着熟悉hexo的各种操作。","categories":[{"name":"正儿八经学技术","slug":"正儿八经学技术","permalink":"kasimg.github.io/categories/%E6%AD%A3%E5%84%BF%E5%85%AB%E7%BB%8F%E5%AD%A6%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"kasimg.github.io/tags/hexo/"}]}]}